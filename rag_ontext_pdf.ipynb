{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drbilal216/AI_201/blob/main/rag_ontext_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependies"
      ],
      "metadata": {
        "id": "9-Zm9fk7UN7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "id": "X2h-lqHBBSyc",
        "outputId": "adef4d33-ea52-410c-f78e-38fe6e6fc602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "4QNWrmOqBS14"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        "Before initializing our vector store, let's connect to a Pinecone index. If one named index_name doesn't exist, it will be created.\n",
        "\n",
        "dimension of embeddings\n",
        "\n",
        "\n",
        "depend on the model we are using for embedding we will use this number\n",
        "eg\n",
        "open ai\n",
        "google"
      ],
      "metadata": {
        "id": "rZIryW9kC2tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-test-index\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768, # Replace with your model dimensions # google = 768 # 3072 ?\n",
        "        metric=\"cosine\", # Replace with your model metric\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "PUw1fCiKC7U0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXoSvm4kh_FF",
        "outputId": "9301b1c8-f1b7-4e23-ace2-0514e5b2bdaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pinecone.data.index.Index object at 0x7a587e7b6710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We also need a pretrained text to embedding genrator"
      ],
      "metadata": {
        "id": "nU1RdGVCOD9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types"
      ],
      "metadata": {
        "id": "4eRrdrqTtpA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "5Wa6jX5gnRX4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",\n",
        "                                          google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Dve13NyxzA_E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vector = embeddings.embed_query(\"hello, world!\")\n",
        "# vector[:5]"
      ],
      "metadata": {
        "id": "ZdvpZwH3vzjD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need these for pinecone\n",
        "# index\n",
        "# embeddings"
      ],
      "metadata": {
        "id": "RCl0KFyY45BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pdf loader"
      ],
      "metadata": {
        "id": "Csd6d0pfW_zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhiN6bHVXDdM",
        "outputId": "92c07774-05e7-45eb-def3-4c8ad91b8873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/298.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNkwg-ARYFKA",
        "outputId": "10425d4b-1ec2-492c-c255-e6b03c26bd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.10.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.16 (from langchain-community)\n",
            "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.32 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.32-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.32-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.4/412.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.31\n",
            "    Uninstalling langchain-core-0.3.31:\n",
            "      Successfully uninstalled langchain-core-0.3.31\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.15\n",
            "    Uninstalling langchain-0.3.15:\n",
            "      Successfully uninstalled langchain-0.3.15\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.16 langchain-community-0.3.16 langchain-core-0.3.32 marshmallow-3.26.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "#file_path\n",
        "file_b = \"b.pdf\"\n",
        "file_m = \"m.pdf\""
      ],
      "metadata": {
        "id": "cNhEm08SXDgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(file_b)\n",
        "cv1 = []\n",
        "async for page in loader.alazy_load():\n",
        "    cv1.append(page)\n",
        "\n",
        "print(f\"{cv1[0].metadata}\\n\")\n",
        "# print(cv1[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_RqfP6JXDjR",
        "outputId": "f95fd9a0-97a3-43d6-81c1-972ae6825911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'b.pdf', 'page': 0, 'page_label': '1'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(file_m)\n",
        "cv2 = []\n",
        "async for page in loader.alazy_load():\n",
        "    cv2.append(page)\n",
        "\n",
        "print(f\"{cv2[0].metadata}\\n\")\n",
        "# print(cv2[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPFluA_NXDmN",
        "outputId": "50e0cbf6-146b-4130-ddd5-0f570953430c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'm.pdf', 'page': 0, 'page_label': '1'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone Vectorstore\n",
        "Now that our Pinecone index is setup, we can initialize our vector store."
      ],
      "metadata": {
        "id": "NkjEQifIbOgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "U-K4Sn9PZ61p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### add items to vector store"
      ],
      "metadata": {
        "id": "A2qzokgrbmPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=(cv1[0].page_content),\n",
        "    metadata=(cv1[0].metadata),\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=(cv2[0].page_content),\n",
        "    metadata=(cv2[0].metadata),\n",
        ")"
      ],
      "metadata": {
        "id": "9nZ3GuGs4yqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [document_1, document_2]"
      ],
      "metadata": {
        "id": "IvzKUAgAhDMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "uuids = [\"1\", \"2\"]"
      ],
      "metadata": {
        "id": "_Fq1u_Vl4yuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload to vector db pine cone\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "id": "M4ppLQY74ywu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2bdba6-5cd2-4994-de1b-12ecc3726481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '2']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete items from vector store\n",
        "# vector_store.delete(ids=[uuids[-1]])"
      ],
      "metadata": {
        "id": "L2MNKhZhkULx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data retreival\n"
      ],
      "metadata": {
        "id": "0JHZZU5JkIA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first way\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    \"mustafa\",\n",
        "    k=1, # data to retreive\n",
        "    # filter={\"source\": \"tweet\"}, #\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "dh1REGPCnRgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ede8ba3-8541-467b-fadb-8006aff048d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Research Papers - Strategist/ Coded\n",
            "GANs over distributed SystemBS in Software Engineering\n",
            "BSE - Bahria University 2020-2024\n",
            "EDUCATION\n",
            "Microsoft Certified: Python Exam(98-381)\n",
            "PIAIC Certified: AI Application Developer\n",
            "CERTIFICATIONS\n",
            "Backend Developer with expertise in SQL, MongoDB, Node.js, and FastAPI. Improved server response times\n",
            "and managed databases. Enhanced user engagement through integrating OpenAI APIs and developed\n",
            "secure smart contracts with Solidity\n",
            "FULLSTACK AI ENGINEER / SOFTWARE ENGINEER\n",
            "+92-336-3739689 · mustafatola02@gmail.com \n",
            "Hussainabad, Karachi, Pakistan\n",
            "Mustufa \n",
            "Machine Learning/ Deep learning - Keras, Tensorflow\n",
            "Data Analysis - Seaborn, Matplotlib, Pandas\n",
            "Data Apps - Streamlit\n",
            "Automation/ Integrations - Selenium, Make.com\n",
            "Database - SQL, MongoDB\n",
            "GENERAL SKILLS\n",
            "Speed Programming - Teknofest Pakistan\n",
            "Took 8th position\n",
            "COMPETITIONS\n",
            "Juniper Networks Global AI Challenge\n",
            "7th Position - AI Planet\n",
            "Backend Developer - Metafourt Technologies\n",
            "Made backend system of freelancing website\n",
            "WORK EXPERIENCE\n",
            "Backend Developer - Softic Solutions\n",
            "Made finance and inventory module\n",
            "h t t p s : / / w w w . l i n k e d i n . c o m / i n / m u s t a f a - t o l a - a 3 a b 1 9 1 b 3 / h t t p s : / / g i t h u b . c o m / m u s t a f a - t o l a \n",
            "h t t p s : / / h u g g i n g f a c e . c o / M u s t u f a \n",
            " h t t p s : / / w w w . k a g g l e . c o m / m u s t a f a t o l a \n",
            "NLP - Chatbots, Semantic analysis, Classificitaion\n",
            "Autoencoders - Cnn based Encoder-Decoder\n",
            "Backend and APIs - FastApi, Flask, NodeJS\n",
            "GenAI - Openai, LLMs, Tools, Agents, LangChain,\n",
            "Llama-index, Vapi, Function-calling, Structured Output\n",
            "Backend Projects\n",
            "Blockchain Protection in AI Datasets - Solidity\n",
            "Cab Booking System - NextJS\n",
            "Airport Parking - NextJS\n",
            "Mobile Apps\n",
            "Smart Energy Meter\n",
            "Built on: Flutter\n",
            "Web Apps\n",
            "PROJECTS/ WORK EXPERIENCE LLM Based Projects\n",
            "Text to Speech\n",
            "Role Playing Agent - based on Personality\n",
            "Conversational Bot - based on VectorDB and LLM\n",
            "Agents: Conversation Retrieval, Pandas Agent \n",
            "Ecommerce - Electronics Store - ReactJS\n",
            "Device Finder - PHP\n",
            "Freelancing Website - MERN\n",
            "ERP Modules - PERN\n",
            "LLM Applications\n",
            "AI Actions - based on Conversation, build on Vapi,\n",
            "Make.com\n",
            "ML Based Projects\n",
            "Intelligent Study Group Matching System using KMeans\n",
            "Stock Market Price Prediction\n",
            "Article Summarizer  [{'page': 0.0, 'page_label': '1', 'source': 'm.pdf'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second way\n",
        "\n",
        "results = vector_store.similarity_search_with_score(\n",
        "    \"What is bilal age\",\n",
        "    k=2,\n",
        "    # filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    # print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")\n",
        "    print(f\"{score:3f}\")\n",
        "    print(f\"{res.page_content}\")\n",
        "    print(\"--------------------------\")"
      ],
      "metadata": {
        "id": "oISHyNgCBoOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c240f447-b99b-4b74-d63a-aa8fa702429c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.585297\n",
            "AI ENGINEER@ THE DISRUPT LABS, PAKISTAN\n",
            "Bilal Muhammad Khan\n",
            "PROFESSIONAL SUMMARY\n",
            "I am an AI engineer specializing in developing advanced algorithms for image processing and deep learning.\n",
            "I have 1.5 years of experience working with computer vision applications.\n",
            "PROJECTS\n",
            "Movie Recommendation System - Machine Learning \n",
            "Implemented K-Nearest Neighbors Algorithm(KNN) on MovieLens Dataset for quick Movie\n",
            "Recommendation and made 2 models, User Based and Item Based. \n",
            "Natural Language Processing (NLP) - Deep Learning \n",
            "Applied advanced deep learning techniques, including Recurrent Neural Networks (RNN), Long\n",
            "Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), with word embedding, to IMDb\n",
            "Movie Reviews dataset for sentiment analysis.\n",
            "House Price Prediction (Regression) - Deep Learning \n",
            "Proficient in regression analysis using deep learning techniques, including application to datasets\n",
            "such as the Boston Housing Dataset. \n",
            "AI-Based Monitoring System - Yunus textile, Pakistan\n",
            "Tracks vehicle movements to ensure compliance with the Weight Bridge process.\n",
            "Validate vehicle number plates in real time.\n",
            "Any violations of this SOP will result in an alert.\n",
            "WORK EXPERIENCE\n",
            "Vehicle Inspection System - Taqat, Saudia\n",
            "Real-time insights into vehicle flow.\n",
            "The Automatic Number Plate Recognition (ANPR) technology identifies and logs each vehicle's\n",
            "license plate.\n",
            "Classifies vehicle types and distinguishing between loading vehicles and regular ones.\n",
            "The system ensures that vehicles entering the factory carry only raw materials, while those\n",
            "existing are loaded with finished goods.\n",
            "AI-Based Person Protective Equipment Compliance - Axens, Saudia\n",
            "Ensuring compliance with safety protocols by monitoring the use of essential personal protective\n",
            "equipment (PPE).\n",
            "A picture with a timestamp of a person not following protocol at the plant entrance will be saved\n",
            "as a violation. This insight is crucial for safety, as it may be important in the event of an incident.\n",
            "Emergency blockage - Unilever, Pakistan\n",
            "Implemented Image processing technique to monitor emergency exit in real time, detecting any\n",
            "obstructions or blockages.\n",
            "Phone:     +92-3452537689\n",
            "Email:       drbilal216@gmail.com \n",
            "Linkedin: www.linkedin.com/in/drbilal216\n",
            "Conducted training of custom YOLO object detection models. \n",
            "Worked with object tracking and pose estimation techniques.\n",
            "CERTIFICATIONS\n",
            "AI For Everyone ( DeepLearning.AI )\n",
            "NN and Deep Learning ( DeepLearning.AI )\n",
            "Improving Deep Neural Networks ( DeepLearning.AI )\n",
            "Data Science_AI Career Bootcamp 2022 ( DataMites )\n",
            "Certified Artificial Intelligence Developer ( PIAIC )\n",
            "Crash Course on Python ( Google )\n",
            "Foundations: Data, Data, Everywhere ( Google )\n",
            "SOFTWARE & SKILLS\n",
            "Data Science, Machine Learning,\n",
            "Deep Learning, Image Processing,\n",
            "Object Detection, Object tracking,\n",
            "Pose Estimation, NLP, Git, Google\n",
            "Colab, Langchain.\n",
            "AI Engineer @ The Disrupt Labs, Pakistan \n",
            "Nov 2023 - Jan 2025 (Onsite) - 1 Year, 2 Months\n",
            "Computer Vision Engineer Intern @ VisionXplore, Pakistan\n",
            "Aug 2023 - Oct 2023 (Hybrid) - 3 months\n",
            "EDUCATION: Bachelor (MCS)\n",
            "CGPA 3.0 Virtual Uni\n",
            "--------------------------\n",
            "0.436500\n",
            "Research Papers - Strategist/ Coded\n",
            "GANs over distributed SystemBS in Software Engineering\n",
            "BSE - Bahria University 2020-2024\n",
            "EDUCATION\n",
            "Microsoft Certified: Python Exam(98-381)\n",
            "PIAIC Certified: AI Application Developer\n",
            "CERTIFICATIONS\n",
            "Backend Developer with expertise in SQL, MongoDB, Node.js, and FastAPI. Improved server response times\n",
            "and managed databases. Enhanced user engagement through integrating OpenAI APIs and developed\n",
            "secure smart contracts with Solidity\n",
            "FULLSTACK AI ENGINEER / SOFTWARE ENGINEER\n",
            "+92-336-3739689 · mustafatola02@gmail.com \n",
            "Hussainabad, Karachi, Pakistan\n",
            "Mustufa \n",
            "Machine Learning/ Deep learning - Keras, Tensorflow\n",
            "Data Analysis - Seaborn, Matplotlib, Pandas\n",
            "Data Apps - Streamlit\n",
            "Automation/ Integrations - Selenium, Make.com\n",
            "Database - SQL, MongoDB\n",
            "GENERAL SKILLS\n",
            "Speed Programming - Teknofest Pakistan\n",
            "Took 8th position\n",
            "COMPETITIONS\n",
            "Juniper Networks Global AI Challenge\n",
            "7th Position - AI Planet\n",
            "Backend Developer - Metafourt Technologies\n",
            "Made backend system of freelancing website\n",
            "WORK EXPERIENCE\n",
            "Backend Developer - Softic Solutions\n",
            "Made finance and inventory module\n",
            "h t t p s : / / w w w . l i n k e d i n . c o m / i n / m u s t a f a - t o l a - a 3 a b 1 9 1 b 3 / h t t p s : / / g i t h u b . c o m / m u s t a f a - t o l a \n",
            "h t t p s : / / h u g g i n g f a c e . c o / M u s t u f a \n",
            " h t t p s : / / w w w . k a g g l e . c o m / m u s t a f a t o l a \n",
            "NLP - Chatbots, Semantic analysis, Classificitaion\n",
            "Autoencoders - Cnn based Encoder-Decoder\n",
            "Backend and APIs - FastApi, Flask, NodeJS\n",
            "GenAI - Openai, LLMs, Tools, Agents, LangChain,\n",
            "Llama-index, Vapi, Function-calling, Structured Output\n",
            "Backend Projects\n",
            "Blockchain Protection in AI Datasets - Solidity\n",
            "Cab Booking System - NextJS\n",
            "Airport Parking - NextJS\n",
            "Mobile Apps\n",
            "Smart Energy Meter\n",
            "Built on: Flutter\n",
            "Web Apps\n",
            "PROJECTS/ WORK EXPERIENCE LLM Based Projects\n",
            "Text to Speech\n",
            "Role Playing Agent - based on Personality\n",
            "Conversational Bot - based on VectorDB and LLM\n",
            "Agents: Conversation Retrieval, Pandas Agent \n",
            "Ecommerce - Electronics Store - ReactJS\n",
            "Device Finder - PHP\n",
            "Freelancing Website - MERN\n",
            "ERP Modules - PERN\n",
            "LLM Applications\n",
            "AI Actions - based on Conversation, build on Vapi,\n",
            "Make.com\n",
            "ML Based Projects\n",
            "Intelligent Study Group Matching System using KMeans\n",
            "Stock Market Price Prediction\n",
            "Article Summarizer \n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### response with LLM"
      ],
      "metadata": {
        "id": "05kHhb7AOm_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        "    )"
      ],
      "metadata": {
        "id": "c01kxA3TRuJq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query: str):\n",
        "  # Vector_Search\n",
        "  vector_results = vector_store.similarity_search(query, k=1)\n",
        "  # Pass to Model Vector result + querry\n",
        "  final_answer = llm.invoke(f\"ANSWER THIS USER QUERRY: {query}, HERE ARE SOME REFERENCES: {vector_results}\")\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "YFC1-yiIoKlT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = answer_to_user(\"What are bilal skillset?\")"
      ],
      "metadata": {
        "id": "_ejP_BoLoKo-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "id": "Qq9cHA55oKs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd684dab-9fb4-4bba-b9f5-e595990697d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided document, Bilal's skillset includes:\n",
            "\n",
            "**Core AI & Machine Learning Skills:**\n",
            "\n",
            "*   **AI Engineering:**  Specializes in developing advanced algorithms for image processing and deep learning.\n",
            "*   **Machine Learning:** Experience with algorithms like K-Nearest Neighbors (KNN).\n",
            "*   **Deep Learning:** Proficient in using deep learning techniques for various tasks, including regression, sentiment analysis, and object detection.\n",
            "*   **Natural Language Processing (NLP):** Experience applying deep learning techniques like RNN, LSTM, and GRU with word embeddings for sentiment analysis.\n",
            "*   **Regression Analysis:** Skilled in using deep learning for regression tasks.\n",
            "*   **Object Detection:**  Experience with custom YOLO object detection models.\n",
            "*   **Object Tracking:** Worked with object tracking techniques.\n",
            "*   **Pose Estimation:**  Experience with pose estimation techniques.\n",
            "\n",
            "**Computer Vision Skills:**\n",
            "\n",
            "*   **Image Processing:**  Strong background in image processing techniques.\n",
            "*   **Computer Vision Applications:** Has 1.5 years of experience working with computer vision applications.\n",
            "\n",
            "**Software & Tools:**\n",
            "\n",
            "*   **Git:** Proficient in using Git for version control.\n",
            "*   **Google Colab:** Experienced using Google Colab for development.\n",
            "*   **Langchain:** Familiar with Langchain.\n",
            "\n",
            "**Programming & Data Science Skills:**\n",
            "\n",
            "*   **Data Science:** General knowledge of data science principles.\n",
            "\n",
            "**Other Relevant Skills (Inferred from Projects & Experience):**\n",
            "\n",
            "*   **Real-time System Development:** Experience developing real-time systems for vehicle inspection, PPE compliance, and emergency blockage detection.\n",
            "*   **Data Analysis:** Experience working with datasets like MovieLens and Boston Housing Dataset.\n",
            "*   **System Implementation:** Experience implementing AI solutions in real-world scenarios (e.g., at Yunus Textile, Taqat, Axens, Unilever).\n",
            "\n",
            "**Certifications:**\n",
            "\n",
            "*   AI For Everyone (DeepLearning.AI)\n",
            "*   NN and Deep Learning (DeepLearning.AI)\n",
            "*   Improving Deep Neural Networks (DeepLearning.AI)\n",
            "*   Data Science_AI Career Bootcamp 2022 (DataMites)\n",
            "*   Certified Artificial Intelligence Developer (PIAIC)\n",
            "*   Crash Course on Python (Google)\n",
            "*   Foundations: Data, Data, Everywhere (Google)\n",
            "\n",
            "**In summary, Bilal is a skilled AI engineer with a strong focus on computer vision and deep learning, possessing both theoretical knowledge and practical experience in developing and implementing AI solutions.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AT5iBQKXS9QL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}