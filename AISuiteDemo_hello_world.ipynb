{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drbilal216/AI_201/blob/main/AISuiteDemo_hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Suite is a light wrapper to provide a unified interface between LLM providers."
      ],
      "metadata": {
        "id": "hZq_yZRcbxdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/andrewyng/aisuite"
      ],
      "metadata": {
        "id": "s4y4Bz1cIU8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuite[all]"
      ],
      "metadata": {
        "id": "1mt8kgFHXMvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7c81e0ed-4a24-4ffa-d2b3-a9382c24f12e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aisuite[all] in /usr/local/lib/python3.11/dist-packages (0.1.7)\n",
            "Requirement already satisfied: anthropic<0.31.0,>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.30.1)\n",
            "Requirement already satisfied: cohere<6.0.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (5.13.8)\n",
            "Requirement already satisfied: groq<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.9.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.59.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (0.9.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.0.20241016)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Pretty Printing Function\n",
        "In this section, we define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting."
      ],
      "metadata": {
        "id": "KwFlLByRbWKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query\n"
      ],
      "metadata": {
        "id": "-Wf7j6abbQmw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "Here we will securely set our API keys as environment variables. This is helpful because we donâ€™t want to hardcode sensitive information (like API keys) directly into our code. By using environment variables, we can keep our credentials secure while still allowing our program to access them. Normally we would use a .env file to store our passwords to our enviroments, but since we are going to be working in colab we will do things a little different."
      ],
      "metadata": {
        "id": "Cce1aLBvctaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GROQ_API_KEY"
      ],
      "metadata": {
        "id": "985OGGh1pb1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "import os\n",
        "# from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY#getpass('Enter your GROQ API key: ')"
      ],
      "metadata": {
        "id": "tluCm7SL0or8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]"
      ],
      "metadata": {
        "id": "hwZi9Mq04nUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "3HvJ3_1a42mD",
        "outputId": "ea4265f9-add9-4bb9-b930-0ef3fcb86a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I assist you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Simple Chat Interaction with an AI Language Model\n",
        "This code initiates a chat interaction with a language model (specifically Groqâ€™s LLaMA 3.2), where the model responds to the user's input. We use the aisuite library to communicate with the model and retrieve the response."
      ],
      "metadata": {
        "id": "m2mhu-VbSWfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a Function to Interact with the Language Model\n",
        "\n",
        "This function, ask, streamlines the process of sending a user message to a language model and retrieving a response. It encapsulates the logic required to set up the conversation and can be reused throughout the notebook for different queries. It will not perserve any history or any continuing conversation.  \n",
        "\n"
      ],
      "metadata": {
        "id": "YJSahowjiJBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"groq:llama-3.2-3b-preview\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "n8DK8_RqqXFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Hi. what is capital of Japan?\")"
      ],
      "metadata": {
        "id": "FGcqY4lBjtFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4b003035-b4da-40fd-cb23-047581264911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Japan is Tokyo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real value of AI Suite is the ablity to run a variety of different models.  Let's first set up a collection of different API keys which we can try out."
      ],
      "metadata": {
        "id": "wpeW6Pj6j_6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confirm each model is using a different provider\n"
      ],
      "metadata": {
        "id": "mfPtlJlbTY6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask(\"Who is your creator?\"))\n",
        "print(ask('Who is your creator?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
        "print(ask('Who is your creator?', model='openai:gpt-4o'))\n"
      ],
      "metadata": {
        "id": "iHVESCGJuWWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "1c6b49f5-509e-4ae4-d20b-349b019a84a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was created by Meta AI, a company that specializes in natural language processing and artificial intelligence. My knowledge was built on a large dataset of text from various sources, and I was trained to generate human-like responses to a wide range of questions and prompts.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-258160f0a246>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Who is your creator?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Who is your creator?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'anthropic:claude-3-5-sonnet-20240620'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Who is your creator?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'openai:gpt-4o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-95dccb28f48f>\u001b[0m in \u001b[0;36mask\u001b[0;34m(message, sys_message, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Send the messages to the model and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Return the content of the model's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aisuite/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Delegate the chat completion to the correct provider's implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mprovider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completions_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aisuite/providers/anthropic_provider.py\u001b[0m in \u001b[0;36mchat_completions_create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         return self.normalize_response(\n\u001b[0;32m---> 31\u001b[0;31m             self.client.messages.create(\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     ) -> Message | Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 904\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 931\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mretries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remaining_retries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_retries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_request\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected JSON data type, {type(json_data)}, cannot merge with `extra_body`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_headers\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mcustom_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mheaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# headers are case-insensitive while dictionaries are not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_client.py\u001b[0m in \u001b[0;36m_validate_headers\u001b[0;34m(self, headers, custom_headers)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;34m'\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Multiple AI Models for a Common Question\n",
        "In this section, we will query several different versions of the LLaMA language model to get varied responses to the same question. This approach allows us to compare how different models handle the same prompt, providing insights into their performance and style."
      ],
      "metadata": {
        "id": "BWBL4D2H2B_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = [\n",
        "    'llama-3.1-8b-instant',\n",
        "    'llama-3.2-1b-preview',\n",
        "    'llama-3.2-3b-preview',\n",
        "    'llama3-70b-8192',\n",
        "    'llama3-8b-8192'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each model\n",
        "ret = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'groq:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(models[idx] + ': \\n ' + x + ' ')"
      ],
      "metadata": {
        "id": "E_gg-sgYuoOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c582ba-3471-4b0e-b9ca-317df8a1c1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('llama-3.1-8b-instant: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the 1956 Dartmouth '\n",
            " 'Summer Research Project on Artificial Intelligence, where a group of '\n",
            " 'computer scientists, led by John McCarthy, Marvin Minsky, Nathaniel '\n",
            " 'Rochester, and Claude Shannon, coined the term and laid the foundation for '\n",
            " 'the development of AI as a distinct field of study. ')\n",
            "('llama-3.2-1b-preview: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the mid-20th '\n",
            " 'century, when the first computer programs, which mimicked human-like '\n",
            " 'intelligence through algorithms and rule-based systems, were developed by '\n",
            " 'renowned mathematicians and computer scientists, including Alan Turing, '\n",
            " 'Marvin Minsky, and John McCarthy in the 1950s. ')\n",
            "('llama-3.2-3b-preview: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the 1950s, with '\n",
            " 'the Dartmouth Summer Research Project on Artificial Intelligence, led by '\n",
            " 'computer scientists John McCarthy, Marvin Minsky, and Nathaniel Rochester, '\n",
            " 'marking the birth of AI as a formal field of research. ')\n",
            "('llama3-70b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientist Alan Turing proposed the Turing Test, a method for '\n",
            " 'determining whether a machine could exhibit intelligent behavior equivalent '\n",
            " 'to, or indistinguishable from, that of a human. ')\n",
            "('llama3-8b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the '\n",
            " '1950s, when computer scientists DARPA funded the development of the first AI '\n",
            " 'programs, such as the Logical Theorist, which aimed to simulate human '\n",
            " 'problem-solving abilities and learn from experience. ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Different AI Providers for a Common Question\n",
        "In this section, we will query multiple AI models from different providers to get varied responses to the same question regarding the origins of AI. This comparison allows us to observe how different models from different architectures respond to the same prompt."
      ],
      "metadata": {
        "id": "Z8pnJPdD2NL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of AI model providers to query\n",
        "providers = [\n",
        "    'groq:llama3-70b-8192',\n",
        "    'openai:gpt-4o',\n",
        "    'anthropic:claude-3-5-sonnet-20240620'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each provider\n",
        "ret = []\n",
        "\n",
        "# Loop through each provider and get a response for the specified question\n",
        "for x in providers:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
        "\n",
        "# Print the provider's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j4TqhC5J1YIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a50e300-0a7a-4562-8a34-f31c4b9072d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('groq:llama3-70b-8192: \\n'\n",
            " 'The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientists like Alan Turing, Marvin Minsky, and John McCarthy '\n",
            " 'began exploring ways to create machines that could think and learn like '\n",
            " 'humans, leading to the development of the first AI programs and '\n",
            " 'algorithms. \\n'\n",
            " '\\n')\n",
            "('openai:gpt-4o: \\n'\n",
            " 'The origins of AI trace back to the mid-20th century, when pioneers like '\n",
            " 'Alan Turing and John McCarthy began exploring the possibility of creating '\n",
            " 'machines that could simulate human intelligence through computational '\n",
            " 'processes. \\n'\n",
            " '\\n')\n",
            "('anthropic:claude-3-5-sonnet-20240620: \\n'\n",
            " 'The origins of AI can be traced back to the 1950s when computer scientists '\n",
            " 'began exploring the concept of creating machines that could simulate human '\n",
            " 'intelligence and problem-solving abilities. \\n'\n",
            " '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating and Evaluating Questions with AI Models\n",
        "In this section, we will randomly generate questions using a language model and then have two other models provide answers to those questions. The user will then evaluate which answer is better, allowing for a comparative analysis of responses from different models."
      ],
      "metadata": {
        "id": "OgPCC0y_U4WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Initialize a list to store the best responses\n",
        "best = []\n",
        "\n",
        "# Loop to generate and evaluate questions\n",
        "for _ in range(20):\n",
        "    # Shuffle the providers list to randomly select models for each iteration\n",
        "    random.shuffle(providers)\n",
        "\n",
        "    # Generate a question using the first provider\n",
        "    question = ask('Please generate a short question that is suitable for asking an LLM.', model=providers[0])\n",
        "\n",
        "    # Get answers from the second and third providers\n",
        "    answer_1 = ask('Please give a short answer to this question: ' + question, model=providers[1])\n",
        "    answer_2 = ask('Please give a short answer to this question: ' + question, model=providers[2])\n",
        "\n",
        "    # Print the generated question and the two answers\n",
        "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
        "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
        "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
        "\n",
        "    # Store the provider names and the user's choice of the best answer\n",
        "    best.append(str(providers) + ', ' + input(\"Which is best 1 or 2. 3 if indistinguishable: \"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fMx-TfLk09ft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56153c03-a1e6-4b72-fd16-b36197ccb5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Original text:\\n'\n",
            " \"  Here's a short question suitable for asking an LLM:\\n\"\n",
            " '\\n'\n",
            " 'What are the potential benefits and risks of artificial intelligence in '\n",
            " 'healthcare?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  **Benefits:**\\n'\n",
            " '1. Improved diagnostics and personalized treatment plans.\\n'\n",
            " '2. Increased efficiency in administrative tasks.\\n'\n",
            " '3. Faster drug discovery and development.\\n'\n",
            " '4. Enhanced patient monitoring and support.\\n'\n",
            " '\\n'\n",
            " '**Risks:**\\n'\n",
            " '1. Privacy and data security concerns.\\n'\n",
            " '2. Potential biases in AI algorithms.\\n'\n",
            " '3. Over-reliance on AI systems by healthcare professionals.\\n'\n",
            " '4. Ethical and accountability issues in decision-making.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  The potential benefits of artificial intelligence (AI) in healthcare '\n",
            " 'include:\\n'\n",
            " '\\n'\n",
            " '* Improved diagnosis accuracy and speed\\n'\n",
            " '* Enhanced patient outcomes through personalized medicine\\n'\n",
            " '* Increased efficiency and reduced costs through automation\\n'\n",
            " '* Better disease prevention and detection\\n'\n",
            " '* Enhanced research capabilities and new treatment discoveries\\n'\n",
            " '\\n'\n",
            " 'However, there are also potential risks, such as:\\n'\n",
            " '\\n'\n",
            " '* Bias in AI decision-making due to flawed data or algorithms\\n'\n",
            " '* Job displacement of healthcare professionals\\n'\n",
            " '* Cybersecurity risks to patient data\\n'\n",
            " '* Dependence on technology leading to deskilling of healthcare workers\\n'\n",
            " '* Unintended consequences of AI-driven decision-making that may not align '\n",
            " 'with human values.\\n'\n",
            " '\\n'\n",
            " 'These benefits and risks highlight the need for responsible development, '\n",
            " 'deployment, and oversight of AI in healthcare.\\n'\n",
            " '\\n')\n",
            "Which is best 1 or 2. 3 if indistinguishable: 3\n",
            "('Original text:\\n'\n",
            " '  What are the potential applications of large language models in '\n",
            " 'healthcare?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  Large language models have numerous potential applications in healthcare, '\n",
            " 'including:\\n'\n",
            " '\\n'\n",
            " '1. **Clinical Decision Support**: Providing doctors with accurate diagnoses, '\n",
            " 'treatment options, and medication recommendations.\\n'\n",
            " '2. **Medical Text Analysis**: Analyzing large amounts of medical literature, '\n",
            " 'patient records, and clinical notes to identify patterns and insights.\\n'\n",
            " '3. **Patient Engagement**: Generating personalized health summaries, '\n",
            " 'communicating medical information in simple language, and facilitating '\n",
            " 'patient-provider communication.\\n'\n",
            " '4. **Disease Surveillance**: Monitoring social media and online platforms '\n",
            " 'for disease outbreaks and tracking epidemiological trends.\\n'\n",
            " '5. **Medical Writing Assistance**: Assisting healthcare professionals in '\n",
            " 'generating medical reports, discharge summaries, and other documents.\\n'\n",
            " '6. **Chatbots and Virtual Assistants**: Offering patients timely support and '\n",
            " 'answers to medical queries.\\n'\n",
            " '7. **Research and Development**: Accelerating biomedical research by '\n",
            " 'analyzing large datasets, identifying research gaps, and suggesting '\n",
            " 'potential areas of investigation.\\n'\n",
            " '\\n'\n",
            " 'These applications have the potential to improve healthcare outcomes, reduce '\n",
            " 'costs, and enhance patient experiences.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  Large language models in healthcare could potentially be used for:\\n'\n",
            " '\\n'\n",
            " '1. Clinical decision support\\n'\n",
            " '2. Medical literature analysis and summarization\\n'\n",
            " '3. Patient triage and symptom checking\\n'\n",
            " '4. Medical education and training\\n'\n",
            " '5. Automated medical coding and documentation\\n'\n",
            " '6. Drug discovery and development\\n'\n",
            " '7. Personalized treatment recommendations\\n'\n",
            " '8. Health-related chatbots for patient engagement\\n'\n",
            " '9. Medical research and hypothesis generation\\n'\n",
            " '10. Natural language processing of electronic health records\\n'\n",
            " '\\n'\n",
            " 'These applications could help improve efficiency, accuracy, and '\n",
            " 'accessibility in various aspects of healthcare.\\n'\n",
            " '\\n')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d17783dc1f7c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store the provider names and the user's choice of the best answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Which is best 1 or 2. 3 if indistinguishable: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI"
      ],
      "metadata": {
        "id": "feOphNMipxof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "twJ-81jIp1dt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]"
      ],
      "metadata": {
        "id": "-O7eNopHrkl3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = \"openai:gpt-3.5-turbo-instruct\"\n",
        "\n",
        "model = \"openai:gpt-4o-mini-2024-07-18\""
      ],
      "metadata": {
        "id": "nxHxbpJeDkxF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=model, messages=messages)"
      ],
      "metadata": {
        "id": "k-4RFhHgCYvF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDjgVKvxBlDa",
        "outputId": "d582ecf7-2abc-4eae-d8e4-07698d0e41d9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IjPvQmDyBlXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}